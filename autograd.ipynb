{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed6922e-fb02-4f70-8e27-dab007dc6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd51cc7d-9dd5-451e-b8cf-2396641c10af",
   "metadata": {},
   "source": [
    "#### 1- Computing Gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5abb8bb-eaea-441b-b884-21595160ad4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7222,  0.8070, -2.0910], requires_grad=True)\n",
      "tensor([ 1.2778,  2.8070, -0.0910], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x0000027503E3A220>\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x + 2\n",
    "print(x)          # Created by the user -> grad_fn is None\n",
    "print(y)          # Created as a result of an operation -> has grad_fn\n",
    "print(y.grad_fn)  # grad_fn references a Function that created the Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad26efd1-aca6-4997-8795-d7a211af6605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.1255, 19.6355, 44.4029], grad_fn=<MulBackward0>)\n",
      "tensor(24.7213, grad_fn=<MeanBackward0>)\n",
      "tensor([3.6743, 5.1167, 7.6944])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "z = z.mean()      # z = (1/len(x)) * sum( 3 * (x+2)**2 )\n",
    "print(z)\n",
    "z.backward()\n",
    "print(x.grad)     # dz/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed3cb7db-6188-4a94-bda5-2e15fcc602c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000e-01, 2.0000e+00, 2.0000e-04])\n"
     ]
    }
   ],
   "source": [
    "# If a Tensor is non-scalar, a gradient argument, which is a tensor of matching shape,\n",
    "# is needed for backward() to calculate the vector-Jacobian product.\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98197fb-a1c0-4aff-b3d7-7278d972bc7d",
   "metadata": {},
   "source": [
    "#### 2- Stopping Gradient Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56afcf2b-5259-4f6e-b826-0bdc1e58d91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "True\n",
      "<SumBackward0 object at 0x000002757BA4D730>\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# For example, during the training loop, the weights must be updated, and this update shouldn't be part of the gradient computation.\n",
    "# 1- x.requires_grad_(False)\n",
    "# 2- x.detach()\n",
    "# 3- wrap in 'with torch.no_grad():'\n",
    "\n",
    "# Example with .requires_grad_(False)\n",
    "a = torch.randn(2, 2)\n",
    "print(a.requires_grad)  # False by default\n",
    "b = ((a * 3) / (a - 1))\n",
    "print(b.grad_fn)        # None, as requires_grad=False\n",
    "a.requires_grad_(True)  # Change requires_grad to True in-place\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)        # Has grad_fn as requires_grad=True\n",
    "\n",
    "# Example with .detach()\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)  # True\n",
    "b = a.detach()          # Create a new tensor detached from the computation graph\n",
    "print(b.requires_grad)  # False\n",
    "\n",
    "# Example with no_grad\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)  # True\n",
    "with torch.no_grad():\n",
    "    print((a ** 2).requires_grad)  # False inside the no_grad context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65809810-ed7c-40cd-af79-ca911b50dd5c",
   "metadata": {},
   "source": [
    "#### 3- Setting the gradients to zero before a new optimization step to prevent accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0fd2f02-b30c-4724-ae96-456e2773f413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if the gradients are not set to zero before a new optimization step:\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n",
      "tensor([-0.8000, -0.8000, -0.8000, -0.8000], requires_grad=True)\n",
      "\n",
      "if the gradients are set to zero before a new optimization step:\n",
      "tensor([12., 12., 12., 12.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([-2.6000, -2.6000, -2.6000, -2.6000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "print(\"if the gradients are not set to zero before a new optimization step:\")\n",
    "for epoch in range(3):\n",
    "    # Dummy model output\n",
    "    model_output = (weights * 3).sum()\n",
    "    model_output.backward()  # Compute gradients\n",
    "    print(weights.grad)      # Print gradients\n",
    "\n",
    "    # Update weights without tracking gradients\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "\n",
    "    # Zero gradients before the next step\n",
    "    #weights.grad.zero_()\n",
    "\n",
    "print(weights)  # Final weights\n",
    "\n",
    "print(\"\\nif the gradients are set to zero before a new optimization step:\")\n",
    "for epoch in range(3):\n",
    "    # Dummy model output\n",
    "    model_output = (weights * 3).sum()\n",
    "    model_output.backward()  # Compute gradients\n",
    "    print(weights.grad)      # Print gradients\n",
    "\n",
    "    # Update weights without tracking gradients\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "\n",
    "    # Zero gradients before the next step\n",
    "    weights.grad.zero_()\n",
    "\n",
    "print(weights)  # Final weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f49c62-ab34-47d9-b3d5-27a7ab7d3d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, use an optimizer which handles zeroing gradients automatically:\n",
    "optimizer = torch.optim.SGD([weights], lr=0.1)\n",
    "\n",
    "for epoch in range(3):\n",
    "    optimizer.zero_grad()       # Zero the gradients\n",
    "    model_output = (weights * 3).sum()\n",
    "    model_output.backward()     # Compute gradients\n",
    "    optimizer.step()            # Update weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
